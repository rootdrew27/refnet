{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import VOCDetection\n",
    "import src.backbones.vgg as vgg\n",
    "import src.models.tinyssd as tinyssd\n",
    "import src.models.helpers as helpers\n",
    "import src.utils.loss as loss\n",
    "\n",
    "from src.models.helpers import multibox_prior, match_anchors_to_gt\n",
    "from src.utils.loss import TinySSDLoss\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils.loss' from 'c:\\\\Users\\\\rooty\\\\OU Research\\\\RefNet\\\\RefNet\\\\RefNet\\\\src\\\\utils\\\\loss.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(vgg)\n",
    "importlib.reload(tinyssd)\n",
    "importlib.reload(helpers) \n",
    "importlib.reload(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./datasets/VOC\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./datasets/VOC\\VOCtrainval_11-May-2012.tar to ./datasets/VOC\n",
      "Using downloaded and verified file: ./datasets/VOC\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./datasets/VOC\\VOCtrainval_11-May-2012.tar to ./datasets/VOC\n"
     ]
    }
   ],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = VOCDetection('./datasets/VOC', image_set='train', download=True, year='2012', transforms=transforms)\n",
    "valset = VOCDetection('./datasets/VOC', image_set='val', download=True, year='2012', transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Pretrianed weights for VGG16\n"
     ]
    }
   ],
   "source": [
    "tinySSD = tinyssd.TinySSD(num_classes=20, imgsz=(224,224), sizes=[[.2,.4, .3], [.6, .5,.7,.8]], ratios=[[1,0.5,2],[1,0.5,2]], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Pretrianed weights for VGG16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 512, 28, 28]), torch.Size([1, 1024, 14, 14])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((1,3,224,224)).to('cuda')\n",
    "y = vgg.VGG16(True).to('cuda')(x)\n",
    "[y.shape for y in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,3,224,224)).to('cuda')\n",
    "y = tinySSD(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 5488, 4]), torch.Size([1, 1568, 4])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in tinySSD.anchors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "    This describes how to combine these tensors of different sizes. We use lists.\n",
    "    Note: this need not be defined in this Class, can be standalone.\n",
    "    :param batch: an iterable of N sets from __getitem__()\n",
    "    :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "    \n",
    "        boxes = [[int(obj['bndbox']['xmin']), int(obj['bndbox']['ymin']), int(obj['bndbox']['xmax']), int(obj['bndbox']['ymax'])] for obj in target['annotation']['object']]\n",
    "        boxes = torch.tensor(boxes)\n",
    "        \n",
    "        cls = torch.as_tensor([1] * len(target['annotation']['object']), dtype=torch.int64)  # Assuming all objects are of class 1\n",
    "        label = torch.cat([cls.view(-1, 1), boxes], dim=1)\n",
    "        \n",
    "        targets.append(label)\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 2\u001b[0m OPTIMIZER \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(tinySSD\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m) \n\u001b[0;32m      3\u001b[0m CRITERION \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m      4\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "OPTIMIZER = torch.optim.SGD(tinySSD.parameters(), lr=0.001, weight_decay=5e-4) \n",
    "CRITERION = TinySSDLoss()\n",
    "DEVICE = 'cuda'\n",
    "DATALOADER = DataLoader(trainset, batch_size=2, drop_last=True, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.5734,  0.5296,  0.9763,  ...,  1.8039,  1.8251,  1.8305],\n",
       "           [ 0.6339,  0.6738,  0.9052,  ...,  1.8386,  1.8193,  1.8071],\n",
       "           [ 0.6040,  0.7154,  1.0721,  ...,  1.8435,  1.8252,  1.8150],\n",
       "           ...,\n",
       "           [-0.6243, -0.6888, -0.6263,  ..., -0.6318, -0.7024, -0.4625],\n",
       "           [-0.4738, -0.3907, -0.5397,  ..., -0.6939, -0.7700, -0.8206],\n",
       "           [-0.5558, -0.4248, -0.4982,  ..., -1.0553, -0.6425, -0.6018]],\n",
       " \n",
       "          [[ 0.0421,  0.0450,  0.3082,  ...,  2.0581,  2.0755,  2.1026],\n",
       "           [ 0.0238,  0.1086,  0.3522,  ...,  2.0386,  2.0462,  2.0516],\n",
       "           [ 0.0940,  0.1292,  0.3932,  ...,  2.0831,  2.0541,  2.0385],\n",
       "           ...,\n",
       "           [-0.5554, -0.5790, -0.4002,  ..., -0.4891, -0.5656, -0.5001],\n",
       "           [-0.7069, -0.5699, -0.6027,  ..., -0.6230, -0.6308, -0.6523],\n",
       "           [-0.5416, -0.4716, -0.4839,  ..., -0.8163, -0.5814, -0.4862]],\n",
       " \n",
       "          [[-0.2386, -0.3848, -0.0206,  ...,  2.3760,  2.3967,  2.4201],\n",
       "           [-0.1090, -0.1807,  0.2116,  ...,  2.3796,  2.3528,  2.3629],\n",
       "           [-0.1323, -0.3283,  0.2768,  ...,  2.3957,  2.3434,  2.3383],\n",
       "           ...,\n",
       "           [-0.4053, -0.3849, -0.3605,  ..., -0.2723, -0.2358, -0.0678],\n",
       "           [-0.3765, -0.2232, -0.3838,  ..., -0.3479, -0.4671, -0.3102],\n",
       "           [-0.2809, -0.2475, -0.3039,  ..., -0.6912, -0.2520, -0.2198]]],\n",
       " \n",
       " \n",
       "         [[[-0.2342, -0.1831, -0.2051,  ..., -0.7235, -0.7224, -0.7479],\n",
       "           [-0.2342, -0.1715, -0.1717,  ..., -0.7325, -0.6829, -0.5927],\n",
       "           [-0.2197, -0.1451, -0.1311,  ..., -0.5657, -0.6291, -0.6778],\n",
       "           ...,\n",
       "           [ 1.0389,  1.1112,  1.0396,  ...,  1.1071,  1.0409,  1.1486],\n",
       "           [ 0.7692,  1.0305,  1.0976,  ...,  1.0624,  1.0443,  1.0491],\n",
       "           [ 0.5364,  0.8556,  1.0139,  ...,  1.1555,  1.1532,  1.2043]],\n",
       " \n",
       "          [[-0.1625, -0.1103, -0.1328,  ..., -0.6649, -0.7141, -0.7402],\n",
       "           [-0.1942, -0.1302, -0.1304,  ..., -0.6423, -0.6581, -0.6133],\n",
       "           [-0.1827, -0.1064, -0.0921,  ..., -0.5701, -0.6306, -0.6445],\n",
       "           ...,\n",
       "           [ 1.2468,  1.3317,  1.2661,  ...,  1.3450,  1.2959,  1.4061],\n",
       "           [ 0.9859,  1.2530,  1.2913,  ...,  1.2860,  1.2748,  1.2797],\n",
       "           [ 0.7479,  1.0742,  1.2057,  ...,  1.3134,  1.2909,  1.3431]],\n",
       " \n",
       "          [[-0.0267,  0.0253,  0.0029,  ..., -0.4798, -0.4887, -0.5147],\n",
       "           [-0.0425,  0.0213,  0.0211,  ..., -0.4865, -0.4764, -0.4199],\n",
       "           [-0.0294,  0.0465,  0.0609,  ..., -0.3860, -0.4462, -0.4689],\n",
       "           ...,\n",
       "           [ 1.5452,  1.6078,  1.5366,  ...,  1.5665,  1.5014,  1.6329],\n",
       "           [ 1.2560,  1.5220,  1.5701,  ...,  1.5638,  1.5627,  1.5676],\n",
       "           [ 1.0191,  1.3439,  1.4849,  ...,  1.6069,  1.5946,  1.6465]]]]),\n",
       " [tensor([[  1,   1, 113, 269, 313],\n",
       "          [  1, 208,  93, 364, 309],\n",
       "          [  1, 192,   1, 379, 299],\n",
       "          [  1, 319,  28, 500, 228]]),\n",
       "  tensor([[  1, 124, 167, 266, 313]])])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DATALOADER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    tinySSD.to(DEVICE)\n",
    "    tinySSD.train()\n",
    "\n",
    "    for i, (imgs, targets) in enumerate(DATALOADER):\n",
    "        imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = tinySSD(imgs)\n",
    "        targets = tinySSD.match(targets) # update targets with matched anchors\n",
    "        loss = CRITERION(outputs, targets)\n",
    "        OPTIMIZER.zero_grad()\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(DATALOADER)}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_boxes = torch.tensor([[[10,20,30,40],[5,10,20,30],[100,100,150,150],[200,200,222,222]]]).to('cuda')\n",
    "# batch size = 1\n",
    "# num gt = 4\n",
    "# num anchors = 5488\n",
    "anchors = tinySSD.anchors\n",
    "\n",
    "ious = helpers.calculate_ious(anchors[0] * 224, gt_boxes)\n",
    "# best_ious, idxs = torch.max(ious, dim=-1)\n",
    "# valid_match_indices = best_ious > 0.5\n",
    "# print(valid_match_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0456, 0.0498, 0.0771,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1194, 0.0374, 0.0664,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1870, 0.0603, 0.0603]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5488, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5488, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors:torch.Tensor = anchors[0].to('cuda')\n",
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs = 2\n",
      "num_gt = 3\n",
      "num_a = 5488\n",
      "torch.Size([2, 3, 5488])\n",
      "torch.Size([2, 3, 5488])\n"
     ]
    }
   ],
   "source": [
    "gt_boxes = torch.tensor([[[10,20,30,40],[100,100,150,150],[200,200,250,250]], [[10,20,30,40],[100,100,150,150],[200,200,250,250]]]).to('cuda')\n",
    "anchors = tinySSD.anchors\n",
    "anchors:torch.Tensor = anchors[0].to('cuda')\n",
    "\n",
    "bs = gt_boxes.shape[0]\n",
    "num_gt = gt_boxes.shape[1]\n",
    "num_a = anchors.shape[0]\n",
    "\n",
    "print(f'bs = {bs}')\n",
    "print(f'num_gt = {num_gt}')\n",
    "print(f'num_a = {num_a}')\n",
    "\n",
    "area_gt_boxes = ((gt_boxes[:, :, 2] - gt_boxes[:, :, 0]) * (gt_boxes[:, :, 3] - gt_boxes[:, :, 1])).view(bs, num_gt, 1).expand(-1, -1, num_a)\n",
    "area_anchors = ((anchors[:, 2] - anchors[:, 0]) * (anchors[:, 3] - anchors[:, 1])).expand(num_gt, -1).expand(bs, -1, -1)\n",
    "\n",
    "\n",
    "\n",
    "# gt_boxes = gt_boxes.view(bs, num_gt, 1, 4).expand(-1, -1, num_a, -1) # shape is (batch_size, num_gt_boxes, num_anchors, 4)\n",
    "# anchors = anchors.expand(num_gt, -1, -1).expand(bs, -1, -1, -1) # shape is (batch_size, num_gt_boxes, num_anchors, 4)\n",
    "\n",
    "print(area_gt_boxes.shape)\n",
    "print(area_anchors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_x1 = torch.max(anchors[:, :, :, 0], gt_boxes[:, :, :, 0]) # shape is (batch_size, num_gt_boxes, num_anchors)\n",
    "inter_y1 = torch.max(anchors[:, :, :, 1], gt_boxes[:, :, :, 1]) # shape is (batch_size, num_gt_boxes, num_anchors)\n",
    "inter_x2 = torch.min(anchors[:, :, :, 2], gt_boxes[:, :, :, 2]) # shape is (batch_size, num_gt_boxes, num_anchors)   \n",
    "inter_y2 = torch.min(anchors[:, :, :, 3], gt_boxes[:, :, :, 3]) # shape is (batch_size, num_gt_boxes, num_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5488])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_width = torch.max(torch.tensor(0.0), inter_x2 - inter_x1) # shape is (batch_size, num_gt_boxes, num_anchors, 1)\n",
    "inter_height = torch.max(torch.tensor(0.0), inter_y2 - inter_y1) # shape is (batch_size, num_gt_boxes, num_anchors, 1)\n",
    "inter_area = inter_width * inter_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5488])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_area.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
